{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNqBxT9goK1R"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogGo_D5m4J5e"
      },
      "source": [
        "! pip install selective_search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1EQUrN7oQ7v"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import selective_search\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms.functional as F\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo3OTCSg3js3"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4_FEDGcxWYp"
      },
      "source": [
        "def crop(image, new_shape):\n",
        "    middle_height = image.shape[2] // 2\n",
        "    middle_width = image.shape[3] // 2\n",
        "    starting_height = middle_height - new_shape[2] // 2\n",
        "    final_height = starting_height + new_shape[2]\n",
        "    starting_width = middle_width - new_shape[3] // 2\n",
        "    final_width = starting_width + new_shape[3]\n",
        "    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n",
        "    return cropped_image"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY70ShBsxQWP"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(3 , 512 , 512)):\n",
        "  image_shifted = image_tensor\n",
        "  image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
        "  image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "  plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "  plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CZu5tdonPj"
      },
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezttTHfJ3nVZ"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self ,\n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) , \n",
        "                 padding = 1 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_pool = False):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels ,\n",
        "                               out_channels ,\n",
        "                               kernel_size , \n",
        "                               stride , \n",
        "                               padding)\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_pool = use_pool\n",
        "\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.ReLU()\n",
        "        if self.use_pool:\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size = (2 , 2) , stride = (2 , 2))\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_pool:\n",
        "            x = self.maxpool(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAVwSSgT3tdh"
      },
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self ,  \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True):\n",
        "        super(Linear , self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_channels , \n",
        "                                 out_channels)\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm1d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.linear1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GFt94ko3w-Z"
      },
      "source": [
        "config = [\n",
        "          # [out_channels , kernel_size , stride , paddin]\n",
        "          [64 , 3 , 1 , 1] , \n",
        "          [128 , 3 , 1 , 1] , \n",
        "          \"M\" , \n",
        "          [128 , 3 , 1 , 1] , \n",
        "          [256 , 3 , 1 , 1] , \n",
        "          \"M\" , \n",
        "          [256 , 3 , 1 , 1] , \n",
        "          [512 , 3 , 1 , 1] , \n",
        "          \"M\" , \n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          \"M\" , \n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          \"M\" , \n",
        "          4096 \n",
        "]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbgmE3E3yvx"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3 , \n",
        "                 config = config):\n",
        "        super(VGG , self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for layer in config:\n",
        "            if isinstance(layer , list):\n",
        "                out_channels , kernel_size , stride , padding = layer\n",
        "                self.layers.append(Conv(\n",
        "                    in_channels , \n",
        "                    out_channels , \n",
        "                    kernel_size , \n",
        "                    stride , \n",
        "                    padding\n",
        "                ))\n",
        "                in_channels = out_channels\n",
        "            elif isinstance(layer , str):\n",
        "                self.layers.append(nn.MaxPool2d(kernel_size = (2 , 2) , stride = (2 , 2)))\n",
        "            else:\n",
        "                if layer == 4096:\n",
        "                    self.layers.append(nn.Flatten())\n",
        "                    self.layers.append(Linear(25088 , 4096))\n",
        "                elif layer == 1000:\n",
        "                    self.layers.append(Linear(4096 , 1000 , use_activation = False))\n",
        "                    self.layers.append(nn.Softmax())\n",
        "    def forward(self , x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlc-fMI131IZ",
        "outputId": "9df083a3-0a65-4adc-af61-84492c7a9da9"
      },
      "source": [
        "x = torch.randn(2 , 3 , 244 , 244).to(device)\n",
        "vgg = VGG().to(device)\n",
        "z = vgg(x)\n",
        "z.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4096])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDC6kYuyoK5s"
      },
      "source": [
        "class Dataset_(torch.utils.data.Dataset):\n",
        "    def __init__(self , \n",
        "                 csv_file , \n",
        "                 img_dir ,\n",
        "                 label_dir , \n",
        "                 transform = None):\n",
        "        super(Dataset_ , self).__init__()\n",
        "\n",
        "        self.csv_file = csv_file\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.df = pd.read_csv(self.csv_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self , idx):\n",
        "        label_path = os.path.join(self.label_dir , self.df.iloc[idx , 1])\n",
        "        boxes = []\n",
        "\n",
        "        with open(label_path) as f:\n",
        "            for label in f.readlines():\n",
        "                class_label , x , y , width , height = [\n",
        "                    float(x) if float(x) != int(float(x)) else int(x)\n",
        "                    for x in label.replace(\"\\n\", \"\").split()\n",
        "                ]\n",
        "                boxes.append([ x , y , width , height , class_label])\n",
        "\n",
        "        target_boxes = torch.tensor(boxes) \n",
        "\n",
        "        img_path = os.path.join(self.img_dir , self.df.iloc[idx , 0])\n",
        "        img = np.asarray(plt.imread(img_path))\n",
        "        img = torch.from_numpy(img).permute(2 , 0 , 1)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        regions = self._get_regions(img)\n",
        "        regions = torch.tensor(regions)\n",
        "        \n",
        "        stacked_croped_images = []\n",
        "        for region in regions:\n",
        "            x , y , w , h = region\n",
        "            img_ = F.crop(img , x , y , w , h)\n",
        "            img_ = F.resize(img_ , (244 , 244))\n",
        "            stacked_croped_images.append(torch.tensor(img_))\n",
        "        return img , target_boxes , regions , stacked_croped_images\n",
        "\n",
        "    def _get_regions(self , image , topN = 2):\n",
        "        image = image.permute(1 , 2 , 0)\n",
        "        boxes = selective_search.selective_search(image, mode='fast')\n",
        "        boxes_filter = selective_search.box_filter(boxes, min_size=20, topN=topN)\n",
        "        return boxes_filter\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD8K0KTRwei6"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToPILImage() , \n",
        "                                transforms.Resize((512 , 512)) , \n",
        "                                transforms.ToTensor()\n",
        "])\n",
        "dataset = Dataset_(\n",
        "    csv_file = \"/content/drive/MyDrive/Yolo_Dataset/train.csv\" , \n",
        "    img_dir = \"/content/drive/MyDrive/Yolo_Dataset/images/\" , \n",
        "    label_dir = \"/content/drive/MyDrive/Yolo_Dataset/labels/\" , \n",
        "    transform = transform\n",
        ")\n",
        "dataloader = torch.utils.data.DataLoader(dataset , batch_size=1 , shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zgA4HPywKW_"
      },
      "source": [
        "for img , trg , regions , imgs in dataloader:\n",
        "    show_tensor_images(img)\n",
        "    print(trg.shape)\n",
        "    print(regions.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB3MRawhAMbT"
      },
      "source": [
        "class RCNN(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3 , \n",
        "                 out_channels = 25):\n",
        "        super(RCNN , self).__init__()\n",
        "\n",
        "        self.backbone = VGG()\n",
        "        self.linear1 = nn.Linear(4096 , out_channels)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.softmax(self.linear1(x))\n",
        "        return x\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRbe72EV_9-L"
      },
      "source": [
        "x = torch.randn(2 , 3 , 244 , 244).to(device)\n",
        "rcnn = RCNN().to(device)\n",
        "z = rcnn(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Gnny-TAfRK"
      },
      "source": [
        "BCE_criterion = nn.BCEWithLogitsLoss()\n",
        "L1_criterion = nn.L1Loss()\n",
        "lambda_recon = 200\n",
        "betas = (0.5 , 0.999)\n",
        "\n",
        "\n",
        "n_epochs = 200\n",
        "in_channels = 3\n",
        "out_channels = 3\n",
        "display_steps = 1\n",
        "lr = 0.0002\n",
        "target_shape = 512"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM2ap0x6HiSi"
      },
      "source": [
        "model = RCNN().to(device)\n",
        "opt = torch.optim.Adam(model.parameters() , lr=lr , betas=betas)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDH0r367VEDg"
      },
      "source": [
        "def train():\n",
        "    mean_rcnn_loss = 0\n",
        "    cur_step = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        for img , target_boxes , regions , croped_imgs in tqdm(dataloader):\n",
        "            cur_batch_size = img.shape[0]\n",
        "            opt.zero_grad()\n",
        "            croped_imgs = torch.stack(croped_imgs)\n",
        "            print(croped_imgs.shape)\n",
        "            print(img.shape)\n",
        "            print(target_boxes.shape)\n",
        "            print(regions.shape)\n",
        "\n",
        "            croped_imgs = croped_imgs.to(device)\n",
        "            img = img.to(device)\n",
        "            target_boxes = target_boxes.to(device)\n",
        "            regions = regions.to(device)\n",
        "\n",
        "            iou_boxes = []\n",
        "            box_len = target_boxes.shape[1]\n",
        "            iou_threshold = 0.5\n",
        "            iou_threshold = torch.tensor([iou_threshold for _ in range(regions.shape[1])]).unsqueeze(0).unsqueeze(2)\n",
        "            for l in range(box_len):\n",
        "                iou = intersection_over_union(regions , target_boxes[: , l , :4].unsqueeze(1))\n",
        "                print(iou.shape)\n",
        "                ious = iou < iou_threshold\n",
        "                ious = ious.int()\n",
        "                iou_boxes.append(ious)\n",
        "            iou_boxes = torch.stack(iou_boxes)\n",
        "            \n",
        "            model_pred = model(croped_imgs.squeeze(1)) # [20 , 25]\n",
        "\n",
        "            obj_loss = L1_criterion(model_pred[... , 24:25] , iou_boxes.squeeze(0).squeeze(0))\n",
        "            co_loss = 0\n",
        "            for l in range(model_pred.shape[0]):\n",
        "                co_loss += L1_criterion(model_pred[l , 20:24] , target_boxes[... , 0:4].squeeze(0))\n",
        "\n",
        "            loss = (obj_loss + co_loss)/2\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            mean_rcnn_loss += loss.item() / display_steps\n",
        "            if cur_step % display_steps == 0:\n",
        "                print(f'Epoch {epoch} , Step {cur_step} , Mean RCNN Loss {mean_rcnn_loss}')\n",
        "            cur_step +=1\n",
        "        mean_rcnn_loss = 0\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veZVxHWwIIKL"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7pzCyEfxNnW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}