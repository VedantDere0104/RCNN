{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_RCNN_With_FPN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqezm2qaseE"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXX9JzjQY6o1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84pwa0VIawLh"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torchvision.ops import roi_pool\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJPjCCu1a1rf"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vdNrLnA8cCK"
      },
      "source": [
        "def iou_width_height(boxes1, boxes2):\n",
        "\n",
        "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "        boxes1[..., 1], boxes2[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ3sJ6hA8eM2"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(3 , 224 , 224)):\n",
        "    image_shifted = image_tensor\n",
        "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njAkT7RA8evI"
      },
      "source": [
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM9gQ-6FkPI1"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self ,\n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) , \n",
        "                 padding = 1 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_pool = False):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels ,\n",
        "                               out_channels ,\n",
        "                               kernel_size , \n",
        "                               stride , \n",
        "                               padding)\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_pool = use_pool\n",
        "\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.ReLU()\n",
        "        if self.use_pool:\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size = (2 , 2) , stride = (2 , 2))\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_pool:\n",
        "            x = self.maxpool(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utkNhbTIa7aH"
      },
      "source": [
        "'''x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "conv = Conv(3 , 32).to(device)\n",
        "z = conv(x)\n",
        "z.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndP41fQTqtsX"
      },
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self ,  \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 use_norm = False , \n",
        "                 use_activation = False):\n",
        "        super(Linear , self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_channels , \n",
        "                                 out_channels)\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm1d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.linear1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naMlJkxikUaQ"
      },
      "source": [
        "class Resnet_Block(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 downsample = False):\n",
        "        super(Resnet_Block , self).__init__()\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if self.downsample:\n",
        "            self.conv1 = Conv(in_channels , \n",
        "                        in_channels , \n",
        "                        kernel_size=(2 , 2) , \n",
        "                        stride=(2 , 2) ,\n",
        "                        padding = 0)\n",
        "            \n",
        "            self.conv_skip = Conv(in_channels ,\n",
        "                            out_channels ,\n",
        "                            kernel_size = (2 ,2) , \n",
        "                            stride = (2 , 2) , \n",
        "                            padding = 0)\n",
        "        else:    \n",
        "            self.conv1 = Conv(in_channels , \n",
        "                            in_channels , \n",
        "                            kernel_size=(1 , 1) , \n",
        "                            stride=(1 , 1) ,\n",
        "                            padding = 0)\n",
        "            \n",
        "            self.conv_skip = Conv(in_channels ,\n",
        "                              out_channels ,\n",
        "                              kernel_size = (1 , 1) , \n",
        "                              stride = (1 ,1) , \n",
        "                              padding = 0)\n",
        "            \n",
        "        self.conv2 = Conv(in_channels , \n",
        "                          in_channels)\n",
        "        \n",
        "        self.conv3 = Conv(in_channels , \n",
        "                          out_channels , \n",
        "                          kernel_size = (1 , 1) , \n",
        "                          stride = (1 , 1) , \n",
        "                          padding = 0)\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self , x): \n",
        "        x_ = x.clone()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x_ = self.conv_skip(x_)\n",
        "        x += x_\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SatDNY1skXlp"
      },
      "source": [
        "'''x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "resnet_block = Resnet_Block(3 , 64 , downsample = True).to(device)\n",
        "z = resnet_block(x)\n",
        "z.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVIQGdm5rqup"
      },
      "source": [
        "class FPN(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 hidden_dim = 256 , \n",
        "                 out_channels = 128):\n",
        "        super(FPN , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , in_channels)\n",
        "        self.conv2 = Conv(in_channels , out_channels)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        return x2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSJGPqYkZY4"
      },
      "source": [
        "class Resnet(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels):\n",
        "        super(Resnet , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , 64 , kernel_size=(7 , 7) , stride=(2 , 2) , padding=3)\n",
        "\n",
        "        self.conv2 = self._make_repeated_blocks(64 , 256 , 3 , downsample = False)\n",
        "        self.conv3 = self._make_repeated_blocks(256 , 512 , 8)\n",
        "        self.conv4 = self._make_repeated_blocks(512 , 1024 , 36)\n",
        "        self.conv5 = self._make_repeated_blocks(1024 , 2048 , 3)\n",
        "        \n",
        "        self.fpn1 = FPN(256)\n",
        "        self.fpn2 = FPN(256)\n",
        "        self.fpn3 = FPN(256)\n",
        "\n",
        "        self.conv1_3 = Conv(2048 , 256)\n",
        "        self.conv1_2 = Conv(1024 , 256)\n",
        "        self.conv1_1 = Conv(512 , 256)\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def _make_repeated_blocks(self , in_channels , out_channels , repeats , downsample = True):\n",
        "        layers = []\n",
        "        for i in range(repeats):\n",
        "            if i == 0 and downsample == True:\n",
        "                layers.append(Resnet_Block(in_channels , out_channels , downsample=downsample))\n",
        "            elif i == 0:\n",
        "                layers.append(Resnet_Block(in_channels , out_channels))\n",
        "            else:\n",
        "                layers.append(Resnet_Block(out_channels , out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.max_pool2d(x , kernel_size = (2 , 2) , stride = (2 , 2))\n",
        "        x = self.conv2(x)\n",
        "        c1 = self.conv3(x)\n",
        "        c2 = self.conv4(c1)\n",
        "        c3 = self.conv5(c2)\n",
        "        #print(c1.shape , c2.shape , c3.shape)\n",
        "        c3 = self.conv1_3(c3)\n",
        "        c2 = self.conv1_2(c2)\n",
        "        c1 = self.conv1_1(c1)\n",
        "        c3_out = self.fpn1(c3)\n",
        "\n",
        "        c3 = self.upsample(c3)\n",
        "        c2_out = self.fpn2(c2 + c3)\n",
        "\n",
        "        c2 = self.upsample(c2)\n",
        "        c1_out = self.fpn3(c1 + c2)\n",
        "        return c3_out , c2_out , c1_out"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83WyzId6kiWj"
      },
      "source": [
        "'''x = torch.randn(2 , 3 , 224 , 224).to(device)\n",
        "resnet = Resnet(3).to(device)\n",
        "c3 , c2 , c1 = resnet(x)\n",
        "c1.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBCFjAHDqc7Z"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels ,\n",
        "                 num_anchors = 3 , \n",
        "                 feature_map_size = 28 , \n",
        "                 num_classes = 2 ):\n",
        "        super(RPN , self).__init__()\n",
        "\n",
        "        self.num_anchors = num_anchors\n",
        "        self.feature_map_size = feature_map_size\n",
        "\n",
        "        out_channels_cls = num_classes * self.feature_map_size ** 2  * num_anchors\n",
        "        out_channels_bbox = 5 * self.num_anchors * self.feature_map_size ** 2 \n",
        "        hidden_dim = in_channels // 2\n",
        "        self.conv1 = Conv(in_channels , hidden_dim , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv2 = Conv(hidden_dim , hidden_dim // 2 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv3 = Conv(hidden_dim // 2 , hidden_dim , kernel_size=(3 , 3) , stride=(1 , 1) , padding=1)\n",
        "        self.conv4 = Conv(hidden_dim , in_channels , kernel_size=(3 , 3) , stride=(1 , 1) , padding=1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        mul_dim = (feature_map_size // 4) ** 2\n",
        "        #mul_dim = mul_dim ** 2\n",
        "        self.linear1 = Linear(128 * mul_dim , 2048)\n",
        "        self.linear2 = Linear(2048 , 1024)\n",
        "\n",
        "        self.linear_cls = Linear(1024 , out_channels_cls)\n",
        "        self.linear_bbox = Linear(1024 , out_channels_bbox)\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        cls = self.linear_cls(x)\n",
        "        bbox = self.linear_bbox(x)\n",
        "        return cls.view(cls.shape[0] , self.num_anchors , self.feature_map_size , self.feature_map_size , 2) , bbox.view(bbox.shape[0] , self.num_anchors   , self.feature_map_size , self.feature_map_size , 5)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMh7msRqqpCb"
      },
      "source": [
        "'''x = torch.randn(2 , 128 , 7 , 7).to(device)\n",
        "rpn = RPN(128 , feature_map_size=7).to(device)\n",
        "cls , bbox = rpn(x)\n",
        "cls.shape , bbox.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v9sCVN8vw6X"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 feature_map_size , \n",
        "                 out_channels_cls = 20 , \n",
        "                 out_channels_bbox = 4 , \n",
        "                 num_anchors = 3):\n",
        "        super(Classifier , self).__init__()\n",
        "    \n",
        "        self.num_anchors = num_anchors\n",
        "        self.feature_map_size = feature_map_size\n",
        "\n",
        "        out_channels_cls = out_channels_cls * self.feature_map_size ** 2  * num_anchors\n",
        "        out_channels_bbox = 5 * self.num_anchors * self.feature_map_size ** 2 \n",
        "        hidden_dim = in_channels // 2\n",
        "        self.conv1 = Conv(in_channels , hidden_dim , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv2 = Conv(hidden_dim , hidden_dim // 2 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv3 = Conv(hidden_dim // 2 , hidden_dim , kernel_size=(3 , 3) , stride=(1 , 1) , padding=1)\n",
        "        self.conv4 = Conv(hidden_dim , in_channels , kernel_size=(3 , 3) , stride=(1 , 1) , padding=1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        mul_dim = (feature_map_size // 4) ** 2\n",
        "        #mul_dim = mul_dim ** 2\n",
        "        self.linear1 = Linear(in_channels * mul_dim , 2048)\n",
        "        self.linear2 = Linear(2048 , 1024)\n",
        "\n",
        "        self.linear_cls = Linear(1024 , out_channels_cls)\n",
        "        self.linear_bbox = Linear(1024 , out_channels_bbox)\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        cls = self.linear_cls(x)\n",
        "        bbox = self.linear_bbox(x)\n",
        "        return cls.view(cls.shape[0] , self.num_anchors , self.feature_map_size , self.feature_map_size , 20) , bbox.view(bbox.shape[0] , self.num_anchors   , self.feature_map_size , self.feature_map_size , 5)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDHXIphPwf7l"
      },
      "source": [
        "'''x = torch.randn(2 , 128 , 28 , 28).to(device)\n",
        "classifier = Classifier(128 , 28).to(device)\n",
        "cls , bbox= classifier(x)\n",
        "print(cls.shape , bbox.shape)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJVUrkGY5Uny"
      },
      "source": [
        "class ROI(nn.Module):\n",
        "    def __init__(self , \n",
        "                 output_len = 2 , \n",
        "                 feature_map_size = 50):\n",
        "        super(ROI , self).__init__()\n",
        "\n",
        "        self.crop_size = feature_map_size\n",
        "        #self.classifier = Classifier(512 , feature_map_size=feature_map_size)\n",
        "        self.output_len = output_len\n",
        "\n",
        "    def forward(self , img , bbox_input , target_boxes):\n",
        "        '''\n",
        "            bbox :- [N x 5 x 50 x 50 x 5]\n",
        "        '''\n",
        "        roi_imgs = []\n",
        "        ious = []\n",
        "        roi_final_imgs = []\n",
        "        for i in range(bbox_input.shape[1]):\n",
        "            for j in range(bbox_input.shape[2]):\n",
        "                for k in range(bbox_input.shape[3]):\n",
        "                    #print(bbox_input[: , i , j , k , :].shape)\n",
        "                    croped_img = roi_pool(img , bbox_input[: , i , j , k , :] , output_size=(self.crop_size , self.crop_size))\n",
        "                    roi_imgs.append(croped_img)\n",
        "                    iou = intersection_over_union(bbox_input[: , i , j , k , :] , target_boxes[: , i , j , k , :])\n",
        "                    ious.append(iou)\n",
        "        roi_imgs = torch.stack(roi_imgs).permute(1 , 0 , 2 , 3 , 4)\n",
        "        ious = torch.stack(ious).permute(1 , 0 , 2)\n",
        "        anchors_indices = ious.argsort(descending=True, dim=1)\n",
        "        \n",
        "        for batch in range(roi_imgs.shape[0]):\n",
        "            for filter in range(roi_imgs.shape[1]):\n",
        "                if filter == self.output_len:\n",
        "                    break\n",
        "                anchor = anchors_indices[batch , filter , :]\n",
        "                roi_img = roi_imgs[batch , anchor , : , : , :]\n",
        "                roi_final_imgs.append(roi_img)\n",
        "        roi_final_imgs = torch.stack(roi_final_imgs)\n",
        "        return roi_final_imgs.squeeze(1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIhECEEh8L57"
      },
      "source": [
        "'''feature_map_size = 7\n",
        "roi = ROI(feature_map_size=feature_map_size).to(device)\n",
        "img = torch.randn(2 , 512 , feature_map_size , feature_map_size).to(device)\n",
        "bbox = torch.randn(2 , 5 , feature_map_size , feature_map_size , 5).to(device)\n",
        "target_boxes = torch.randn(2 , 5 , feature_map_size , feature_map_size , 5).to(device)\n",
        "roi_imgs = roi(img , bbox , target_boxes)\n",
        "roi_imgs.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiRQnIylvGPK"
      },
      "source": [
        "class FasterRCNN_FPN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FasterRCNN_FPN , self).__init__()\n",
        "\n",
        "        self.resnet = Resnet(3)\n",
        "\n",
        "        self.rpn_3 = RPN(128 , feature_map_size=28)\n",
        "        self.rpn_2 = RPN(128 , feature_map_size=14)\n",
        "        self.rpn_1 = RPN(128 , feature_map_size=7)\n",
        "\n",
        "        self.classifier_3 = Classifier(128 , feature_map_size=28)\n",
        "        self.classifier_2 = Classifier(128 , feature_map_size=14)\n",
        "        #self.classifier_1 = Classifier(128 , feature_map_size=7)\n",
        "\n",
        "        self.roi_3 = ROI(feature_map_size=28)\n",
        "        self.roi_2 = ROI(feature_map_size=14)\n",
        "        #self.roi_1 = ROI(feature_map_size=7)\n",
        "\n",
        "    def forward(self , x , target_boxes):\n",
        "        x1 , x2 , x3 = self.resnet(x)\n",
        "        #print(x1.shape)\n",
        "        x3_cls , x3_bbox = self.rpn_3(x3)\n",
        "        x2_cls , x2_bbox = self.rpn_2(x2)\n",
        "        #x1_cls , x1_bbox = self.rpn_1(x1)\n",
        "        #print(x1.shape , x1_bbox.shape)\n",
        "        img_3 = self.roi_3(x3 , x3_bbox , target_boxes[0])\n",
        "        img_2 = self.roi_2(x2 , x2_bbox , target_boxes[1])\n",
        "        #img_1 = self.roi_1(x1 , x1_bbox , target_boxes[2])\n",
        "\n",
        "        out_3 = self.classifier_3(img_3)\n",
        "        out_2 = self.classifier_2(img_2)\n",
        "        #out_1 = self.classifier_1(img_1)\n",
        "        out_3_cls , out_3_bbox = out_3\n",
        "        out_2_cls , out_2_bbox = out_2\n",
        "        return out_3_cls , out_3_bbox , out_2_cls , out_2_bbox"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL_ls05Q0uUK"
      },
      "source": [
        "'''x = torch.randn(1 , 3 , 224 , 224).to(device)\n",
        "faster_rcnn_fpn = FasterRCNN_FPN().to(device)\n",
        "target_boxes = [torch.randn(1 , 3 , 28 , 28 , 5).to(device) , \n",
        "                torch.randn(1 , 3 , 14 , 14 , 5).to(device)]\n",
        "out_3_cls , out_3_bbox , out_2_cls , out_2_bbox = faster_rcnn_fpn(x , target_boxes)\n",
        "out_3_cls.shape , out_3_bbox.shape , out_2_cls.shape , out_2_bbox.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1p7-x4ZI5-b"
      },
      "source": [
        "anchors = [\n",
        "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYSaPPBS8VGa"
      },
      "source": [
        "class Dataset_(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_file,\n",
        "        img_dir,\n",
        "        label_dir,\n",
        "        anchors = anchors,\n",
        "        S=[14, 28, 56],\n",
        "        C=20,\n",
        "        transform=None,\n",
        "    ):\n",
        "        self.df = pd.read_csv(csv_file)[:10]\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
        "        self.num_anchors = self.anchors.shape[0]\n",
        "        self.num_anchors_per_scale = self.num_anchors // 3\n",
        "        #print(self.num_anchors_per_scale , self.num_anchors)\n",
        "        self.C = C\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self , idx):\n",
        "        label_path = os.path.join(self.label_dir , self.df.iloc[idx , 1])\n",
        "        boxes = []\n",
        "\n",
        "        with open(label_path) as f:\n",
        "            for label in f.readlines():\n",
        "                class_label , x , y , width , height = [\n",
        "                    float(x) if float(x) != int(float(x)) else int(x)\n",
        "                    for x in label.replace(\"\\n\", \"\").split()\n",
        "                ]\n",
        "                boxes.append([ x , y , width , height , class_label])\n",
        "\n",
        "        boxes = torch.tensor(boxes) \n",
        "\n",
        "        image_path = os.path.join(self.img_dir , self.df.iloc[idx , 0])\n",
        "        image = np.asarray(plt.imread(image_path))\n",
        "        image = torch.from_numpy(image).permute(2 , 0 , 1)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        target = [torch.zeros((self.num_anchors // 3 , S , S , 6)) for S in self.S]\n",
        "\n",
        "        for box in boxes:\n",
        "            iou_anchors = iou_width_height(box[2:4] , self.anchors)\n",
        "            anchors_indices = iou_anchors.argsort(descending = True , dim = 0)\n",
        "\n",
        "            x , y , width , height , class_label = box\n",
        "            has_anchor = [False] * 3\n",
        "            for anchor_idx in anchors_indices:\n",
        "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "                S = self.S[scale_idx]\n",
        "                i , j = int(S * y) , int(S * x)\n",
        "                anchor_taken = target[scale_idx][anchor_on_scale , i , j , 0]\n",
        "                if not anchor_taken and not has_anchor[scale_idx] :\n",
        "                    target[scale_idx][anchor_on_scale, i , j , 0] = 1\n",
        "                    x_cell , y_cell = S * x - j , S * y - i\n",
        "                    width_cell , height_cell = (\n",
        "                        width * S , \n",
        "                        height * S\n",
        "                    )\n",
        "                    box_coordinates = torch.tensor(\n",
        "                        [x_cell , y_cell , width_cell , height_cell]\n",
        "                    )\n",
        "                    target[scale_idx][anchor_on_scale , i , j , 1:5] = box_coordinates\n",
        "                    target[scale_idx][anchor_on_scale , i , j , 5] = int(class_label)\n",
        "                    has_anchor[scale_idx] = True\n",
        "\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "                    target[scale_idx][anchor_on_scale , i , j , 0] = -1\n",
        "        return image , tuple(target)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRtqhwo_I1zW"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToPILImage() , \n",
        "                                transforms.Resize((224 , 224)) , \n",
        "                                transforms.ToTensor()\n",
        "])\n",
        "dataset = Dataset_(\n",
        "    img_dir = '/content/drive/MyDrive/Yolo_Dataset/images/' , \n",
        "    label_dir = '/content/drive/MyDrive/Yolo_Dataset/labels' , \n",
        "    csv_file = '/content/drive/MyDrive/Yolo_Dataset/train.csv' , \n",
        "    transform = transform\n",
        ")\n",
        "dataloader = torch.utils.data.DataLoader(dataset , batch_size = 1 , shuffle=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GatNnxUFI7uj"
      },
      "source": [
        "for x , y in dataloader:\n",
        "    show_tensor_images(x)\n",
        "    print(y[0].shape , y[1].shape , y[2].shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxnQqOG6QO7e"
      },
      "source": [
        "adv_criterion = nn.BCEWithLogitsLoss()\n",
        "l1_criterion = nn.L1Loss()\n",
        "lambda_recon = 200\n",
        "betas = (0.5 , 0.999)\n",
        "\n",
        "\n",
        "n_epochs = 200\n",
        "display_step = 1\n",
        "lr = 0.0002\n",
        "target_shape = 512"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEV5hViESQBK"
      },
      "source": [
        "faster_rcnn_fpn = FasterRCNN_FPN().to(device)\n",
        "opt = torch.optim.Adam(faster_rcnn_fpn.parameters() , lr=lr , betas = betas)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVwImqIPSfeo"
      },
      "source": [
        "def train():\n",
        "    mean_rcnn_loss = 0\n",
        "    cur_step = 0\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        for img , label in dataloader:\n",
        "            img = img.to(device)\n",
        "            label_1 , label_2 , _ = label\n",
        "            label_1 = label_1.to(device)\n",
        "            label_2 = label_2.to(device)\n",
        "            target = [label_2[... , 0:5] , \n",
        "                      label_1[... , 0:5]]\n",
        "            opt.zero_grad()\n",
        "            #print(target[0].shape , target[1].shape)\n",
        "            out_1_cls , out_1_bbox , out_2_cls , out_2_bbox = faster_rcnn_fpn(img , target)\n",
        "            #print(out_1_cls.shape , out_1_cls.shape)    \n",
        "            #print(label_2.shape)\n",
        "            for cls_1 , bbox_1 , cls_2 , bbox_2 in zip(out_1_cls , out_1_bbox , out_2_cls , out_2_bbox):\n",
        "                loss_1 = l1_criterion(bbox_1.unsqueeze(0) , label_2[..., :5])\n",
        "                loss_2 = l1_criterion(bbox_2.unsqueeze(0) , label_1[..., :5])\n",
        "                #print(cls_1.shape , label_2[... , 5:6].shape)\n",
        "                class_loss_1 = l1_criterion(cls_1 , label_2[... , 5:6])\n",
        "                class_loss_2 = l1_criterion(cls_2 , label_1[... , 5:6])\n",
        "                loss = (loss_1 + loss_2 + class_loss_1 + class_loss_2)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            mean_rcnn_loss += loss.item() / display_steps\n",
        "            if cur_step % display_steps == 0:\n",
        "                print(f'Epoch {epoch} , Step {cur_step} , Mean YOLO Loss {mean_rcnn_loss}')\n",
        "            cur_step +=1\n",
        "        mean_rcnn_loss = 0"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvEY9UqJUeY1"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv3-UFbSUe5k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}