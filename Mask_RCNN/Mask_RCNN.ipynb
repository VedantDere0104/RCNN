{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask_RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PexTm_Y30NLN"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2tLxPDd0TAR"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.ops import roi_align"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KFWeIRG0gwy"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V33Hh6Hv0px-"
      },
      "source": [
        "def iou_width_height(boxes1, boxes2):\n",
        "\n",
        "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "        boxes1[..., 1], boxes2[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHhH0Jg30sau"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(3 , 800 , 800)):\n",
        "    image_shifted = image_tensor\n",
        "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOQK8U5G0uQm"
      },
      "source": [
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWfR58wo09XG"
      },
      "source": [
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self ,\n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) , \n",
        "                 padding = 1 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_pool = False):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels ,\n",
        "                               out_channels ,\n",
        "                               kernel_size , \n",
        "                               stride , \n",
        "                               padding)\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_pool = use_pool\n",
        "\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.ReLU()\n",
        "        if self.use_pool:\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size = (2 , 2) , stride = (2 , 2))\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_pool:\n",
        "            x = self.maxpool(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_qbi7GRgHHn"
      },
      "source": [
        "\n",
        "class ConvT(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels, \n",
        "                 kernel_size = (2 , 2) , \n",
        "                 stride = (2 , 2) , \n",
        "                 padding = 0 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True):\n",
        "        super(ConvT , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels ,\n",
        "                                         out_channels , \n",
        "                                        kernel_size , \n",
        "                                        stride , \n",
        "                                        padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.convT(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYaJN1f40_jW"
      },
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self ,  \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 use_norm = False , \n",
        "                 use_activation = True):\n",
        "        super(Linear , self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_channels , \n",
        "                                 out_channels)\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm1d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.linear1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWXelDhP1B6I"
      },
      "source": [
        "config = [\n",
        "          # [out_channels , kernel_size , stride , paddin]\n",
        "          [64 , 3 , 1 , 1] , \n",
        "          [128 , 3 , 1 , 1] , \n",
        "          \"M\" , \n",
        "          [128 , 3 , 1 , 1] , \n",
        "          [256 , 3 , 1 , 1] , \n",
        "          \"M\" , \n",
        "          [256 , 3 , 1 , 1] , \n",
        "          [512 , 3 , 1 , 1] , \n",
        "          \"M\" , \n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          \"M\" , \n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          [512 , 3 , 1 , 1] ,\n",
        "          #\"M\" , \n",
        "          #4096 \n",
        "]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH7nxQtZ1DzI"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3 , \n",
        "                 config = config):\n",
        "        super(VGG , self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for layer in config:\n",
        "            if isinstance(layer , list):\n",
        "                out_channels , kernel_size , stride , padding = layer\n",
        "                self.layers.append(Conv(\n",
        "                    in_channels , \n",
        "                    out_channels , \n",
        "                    kernel_size , \n",
        "                    stride , \n",
        "                    padding\n",
        "                ))\n",
        "                in_channels = out_channels\n",
        "            elif isinstance(layer , str):\n",
        "                self.layers.append(nn.MaxPool2d(kernel_size = (2 , 2) , stride = (2 , 2)))\n",
        "            else:\n",
        "                if layer == 4096:\n",
        "                    self.layers.append(nn.Flatten())\n",
        "                    self.layers.append(Linear(25088 , 4096))\n",
        "                elif layer == 1000:\n",
        "                    self.layers.append(Linear(4096 , 1000 , use_activation = False))\n",
        "                    self.layers.append(nn.Softmax())\n",
        "    def forward(self , x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy7Yhr1l1GJe"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels ,\n",
        "                 num_anchors = 5 , \n",
        "                 feature_map_size = 50 , \n",
        "                 num_classes = 2):\n",
        "        super(RPN , self).__init__()\n",
        "\n",
        "        self.num_anchors = num_anchors\n",
        "        self.feature_map_size = feature_map_size\n",
        "\n",
        "        out_channels_cls = num_classes * self.feature_map_size ** 2  * num_anchors\n",
        "        out_channels_bbox = 5 * self.num_anchors * self.feature_map_size ** 2 \n",
        "        hidden_dim = in_channels // 2\n",
        "        self.conv1 = Conv(in_channels , hidden_dim , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv2 = Conv(hidden_dim , hidden_dim //2 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv3 = Conv(hidden_dim // 2 , hidden_dim , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv4 = Conv(hidden_dim , in_channels , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear1 = Linear(4608 , 2048)\n",
        "        self.linear2 = Linear(2048 , 1024)\n",
        "\n",
        "        self.linear_cls = Linear(1024 , out_channels_cls)\n",
        "        self.linear_bbox = Linear(1024 , out_channels_bbox)\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        cls = self.linear_cls(x)\n",
        "        bbox = self.linear_bbox(x)\n",
        "        return cls.view(cls.shape[0] , 2 , self.feature_map_size , self.feature_map_size , 5) , bbox.view(bbox.shape[0] , self.num_anchors   , self.feature_map_size , self.feature_map_size , 5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozSV7rGo1JRG"
      },
      "source": [
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels_cls = 20 , \n",
        "                 out_channels_bbox = 4 , \n",
        "                 num_anchors = 5):\n",
        "        super(Classifier , self).__init__()\n",
        "\n",
        "        out_channels_bbox = out_channels_bbox * num_anchors * 50 * 50\n",
        "        out_channels_cls = out_channels_cls * num_anchors * 50 * 50\n",
        "        self.conv1 = Conv(in_channels , in_channels // 2 , use_pool=True)\n",
        "        self.conv2 = Conv(in_channels //2 , in_channels // 4 , use_pool = True)\n",
        "        self.conv3 = Conv(in_channels // 4 , in_channels // 8 , use_pool = False)\n",
        "        self.conv4 = Conv(in_channels // 8 , in_channels // 16 , use_pool=False)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = Linear(32 , 256)\n",
        "        self.linear2 = Linear(256 , 128)\n",
        "\n",
        "        self.linear_cls = Linear(128 , out_channels_cls)\n",
        "        self.linear_bbox = Linear(128 , out_channels_bbox)\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        #print(x.shape)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        cls = self.linear_cls(x)\n",
        "        bbox = self.linear_bbox(x)\n",
        "        #print(cls.shape , bbox.shape)\n",
        "        return cls.view(x.shape[0] ,  5 , 50 , 50 , 20) , bbox.view(x.shape[0] ,  5 , 50 , 50 , 4)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJE7SC8c1M4m"
      },
      "source": [
        "x = torch.randn(2 , 512 , 5 , 5).to(device)\n",
        "cls = Classifier(512).to(device)\n",
        "cls_ , bbox = cls(x)\n",
        "cls_.shape , bbox.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUwmAKQj1PFu"
      },
      "source": [
        "class Dataset_(torch.utils.data.Dataset):\n",
        "    def __init__(self ,\n",
        "                 img_dir , \n",
        "                 label_dir , \n",
        "                 csv_file , \n",
        "                 anchors , \n",
        "                 transforms = None , \n",
        "                 S = 50 , \n",
        "                 B = 5 , \n",
        "                 C = 20):\n",
        "        super(Dataset_ , self).__init__()\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.anchors = torch.from_numpy(np.array(anchors))\n",
        "        #print(self.anchors)\n",
        "        self.transforms = transforms\n",
        "        self.number_of_anchors_per_cell = 5\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "        self.C = C\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.mask_size = 5\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_size = 800\n",
        "        label_path = os.path.join(self.label_dir , self.df.iloc[idx , 1])\n",
        "        boxes = []\n",
        "        binary_mask = []\n",
        "        label_mask = []\n",
        "\n",
        "        img_path = os.path.join(self.img_dir , self.df.iloc[idx , 0])\n",
        "        image = np.asarray(plt.imread(img_path))\n",
        "        image = torch.from_numpy(image).permute(2 , 0 , 1)\n",
        "        transform_mask = transforms.Compose([\n",
        "                                             transforms.ToPILImage() , \n",
        "                                             transforms.Resize((5 , 5)) , \n",
        "                                             transforms.Grayscale() , \n",
        "                                             transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        with open(label_path) as f:\n",
        "            for label in f.readlines():\n",
        "                class_label , x , y , width , height = [\n",
        "                    float(x) if float(x) != int(float(x)) else int(x)\n",
        "                    for x in label.replace(\"\\n\", \"\").split()\n",
        "                ]\n",
        "                boxes.append([ x , y , width , height , class_label])\n",
        "                '''\n",
        "                i_ , j_ = int(img_size * y) , int(img_size * x)\n",
        "                x = img_size * x - j_\n",
        "                y = img_size * y - i_\n",
        "                height = img_size * height \n",
        "                width = img_size * width \n",
        "                img_ = F.crop(image , int(x) , int(y) , int(width) , int(height))\n",
        "                img_ = transform_mask(img_)\n",
        "                binary_mask.append(img_)\n",
        "                label_mask.append(class_label)\n",
        "                '''\n",
        "        #label_mask = torch.tensor(label_mask)\n",
        "        boxes = torch.tensor(boxes) \n",
        "        #binary_mask = torch.stack(binary_mask)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        targets = torch.zeros((self.B , self.S , self.S , 5))\n",
        "        target_mask = torch.zeros((self.B , self.mask_size , self.mask_size , 2))\n",
        "        for box in boxes:\n",
        "            iou_anchors = iou_width_height(box[2:4] , self.anchors)\n",
        "            anchors_indices = iou_anchors.argsort(descending=True, dim=0)        \n",
        "            x , y , width , height , class_label = box\n",
        "            has_anchor = [False for _ in range(self.B)]\n",
        "            for anchor_idx in anchors_indices:\n",
        "                anchor_on_scale = anchor_idx % self.B\n",
        "                S = self.S\n",
        "                i , j = int(S * y) , int(S * x)\n",
        "                anchor_taken = targets[anchor_on_scale , i , j , 0]\n",
        "                if not anchor_taken and not has_anchor[anchor_on_scale]:\n",
        "                    targets[anchor_on_scale , i , j , 0] = 1\n",
        "                    x_cell , y_cell = S * x - j , S * y - i\n",
        "                    width_cell , height_cell = (\n",
        "                        width * S , \n",
        "                        height * S\n",
        "                    )\n",
        "                    box_coordinate = torch.tensor([x_cell , y_cell , width_cell , height_cell])\n",
        "                    targets[anchor_on_scale , i , j , :4] = box_coordinate\n",
        "                    targets[anchor_on_scale , i , j , 4] = int(class_label)\n",
        "                    target_mask_ = F.crop(image , int(x_cell) , int(y_cell) , int(width_cell) , int(height_cell))\n",
        "                    target_mask_ = transform_mask(target_mask_)\n",
        "                    #print(target_mask_.shape)\n",
        "                    target_mask[anchor_on_scale] = target_mask_.permute(1 , 2 , 0)\n",
        "                    #target_mask[anchor_on_scale , i , j , 1] = int(class_label)\n",
        "                    has_anchor[anchor_on_scale] = True\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "                    targets[anchor_on_scale , i , j , 0] = -1\n",
        "        return image , targets , target_mask"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPtRLBdftugb"
      },
      "source": [
        "class Dataset_(torch.utils.data.Dataset):\n",
        "    def __init__(self ,\n",
        "                 img_dir , \n",
        "                 label_dir , \n",
        "                 csv_file , \n",
        "                 anchors , \n",
        "                 transforms = None , \n",
        "                 S = 50 , \n",
        "                 B = 5 , \n",
        "                 C = 20):\n",
        "        super(Dataset_ , self).__init__()\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.anchors = torch.from_numpy(np.array(anchors))\n",
        "        #print(self.anchors)\n",
        "        self.transforms = transforms\n",
        "        self.number_of_anchors_per_cell = 5\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "        self.C = C\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.mask_size = 5\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_size = 800\n",
        "        label_path = os.path.join(self.label_dir , self.df.iloc[idx , 1])\n",
        "        boxes = []\n",
        "        binary_mask = []\n",
        "        label_mask = []\n",
        "\n",
        "        img_path = os.path.join(self.img_dir , self.df.iloc[idx , 0])\n",
        "        image = np.asarray(plt.imread(img_path))\n",
        "        image = torch.from_numpy(image).permute(2 , 0 , 1)\n",
        "        transform_mask = transforms.Compose([\n",
        "                                             transforms.ToPILImage() , \n",
        "                                             transforms.Resize((5 , 5)) , \n",
        "                                             transforms.Grayscale() , \n",
        "                                             transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        with open(label_path) as f:\n",
        "            for label in f.readlines():\n",
        "                class_label , x , y , width , height = [\n",
        "                    float(x) if float(x) != int(float(x)) else int(x)\n",
        "                    for x in label.replace(\"\\n\", \"\").split()\n",
        "                ]\n",
        "                boxes.append([ x , y , width , height , class_label])\n",
        "                '''\n",
        "                i_ , j_ = int(img_size * y) , int(img_size * x)\n",
        "                x = img_size * x - j_\n",
        "                y = img_size * y - i_\n",
        "                height = img_size * height \n",
        "                width = img_size * width \n",
        "                img_ = F.crop(image , int(x) , int(y) , int(width) , int(height))\n",
        "                img_ = transform_mask(img_)\n",
        "                binary_mask.append(img_)\n",
        "                label_mask.append(class_label)\n",
        "                '''\n",
        "        #label_mask = torch.tensor(label_mask)\n",
        "        boxes = torch.tensor(boxes) \n",
        "        #binary_mask = torch.stack(binary_mask)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        targets = torch.zeros((self.B , self.S , self.S , 5))\n",
        "        target_mask = torch.zeros((self.B , self.S , self.S , 2))\n",
        "        for box in boxes:\n",
        "            iou_anchors = iou_width_height(box[2:4] , self.anchors)\n",
        "            anchors_indices = iou_anchors.argsort(descending=True, dim=0)        \n",
        "            x , y , width , height , class_label = box\n",
        "            has_anchor = [False for _ in range(self.B)]\n",
        "            for anchor_idx in anchors_indices:\n",
        "                anchor_on_scale = anchor_idx % self.B\n",
        "                S = self.S\n",
        "                i , j = int(S * y) , int(S * x)\n",
        "                anchor_taken = targets[anchor_on_scale , i , j , 0]\n",
        "                if not anchor_taken and not has_anchor[anchor_on_scale]:\n",
        "                    targets[anchor_on_scale , i , j , 0] = 1\n",
        "                    x_cell , y_cell = S * x - j , S * y - i\n",
        "                    width_cell , height_cell = (\n",
        "                        width * S , \n",
        "                        height * S\n",
        "                    )\n",
        "                    box_coordinate = torch.tensor([x_cell , y_cell , width_cell , height_cell])\n",
        "                    targets[anchor_on_scale , i , j , :4] = box_coordinate\n",
        "                    targets[anchor_on_scale , i , j , 4] = int(class_label)\n",
        "                    target_mask_ = F.crop(image , int(x_cell) , int(y_cell) , int(width_cell) , int(height_cell))\n",
        "                    target_mask_ = transform_mask(target_mask_)\n",
        "                    #print(target_mask_.permute(1 , 2 , 0).shape)\n",
        "                    #print(target_mask[anchor_on_scale , i:i+5 , j:j+5 , 0:1].shape)\n",
        "                    target_mask[anchor_on_scale , i:i+5 , j:j+5 , 0:1] = target_mask_.permute(1 , 2 , 0)\n",
        "                    target_mask[anchor_on_scale , i:i+5 , j:j+5 , 1:2] = int(class_label)\n",
        "                    has_anchor[anchor_on_scale] = True\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "                    targets[anchor_on_scale , i , j , 0] = -1\n",
        "        return image , targets , target_mask"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1jrUp9z1dLj"
      },
      "source": [
        "anchors = [[ 0.28, 0.22], [  0.38, 0.48], [ 0.9, 0.78], [ 0.07, 0.15], [ 0.15, 0.11]]\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToPILImage() , \n",
        "                                transforms.Resize((800 , 800)) , \n",
        "                                transforms.ToTensor()\n",
        "])\n",
        "dataset = Dataset_(\n",
        "    img_dir = '/content/drive/MyDrive/Yolo_Dataset/images/' , \n",
        "    label_dir = '/content/drive/MyDrive/Yolo_Dataset/labels' , \n",
        "    csv_file = '/content/drive/MyDrive/Yolo_Dataset/train.csv' , \n",
        "    anchors = anchors , \n",
        "    transforms = transform\n",
        ")\n",
        "dataloader = torch.utils.data.DataLoader(dataset , batch_size = 1 , shuffle=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90uZnYL01hNL"
      },
      "source": [
        "for x , y , z in dataloader:\n",
        "    show_tensor_images(x)\n",
        "    print(y.shape)\n",
        "    print(z.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5lKqvSd1ivu"
      },
      "source": [
        "class ROI(nn.Module):\n",
        "    def __init__(self , \n",
        "                 output_len = 10):\n",
        "        super(ROI , self).__init__()\n",
        "\n",
        "        self.crop_size = 5\n",
        "        self.classifier = Classifier(512)\n",
        "        self.output_len = output_len\n",
        "\n",
        "    def forward(self , img , bbox_input , target_boxes):\n",
        "        '''\n",
        "            bbox :- [N x 5 x 50 x 50 x 5]\n",
        "        '''\n",
        "        roi_imgs = []\n",
        "        ious = []\n",
        "        roi_final_imgs = []\n",
        "        for i in range(bbox_input.shape[1]):\n",
        "            for j in range(bbox_input.shape[2]):\n",
        "                for k in range(bbox_input.shape[3]):\n",
        "                    #print(bbox_input[: , i , j , k , :].shape)\n",
        "                    croped_img = roi_align(img , bbox_input[: , i , j , k , :] , output_size=(self.crop_size , self.crop_size))\n",
        "                    roi_imgs.append(croped_img)\n",
        "                    iou = intersection_over_union(bbox_input[: , i , j , k , :] , target_boxes[: , i , j , k , :])\n",
        "                    ious.append(iou)\n",
        "        roi_imgs = torch.stack(roi_imgs).permute(1 , 0 , 2 , 3 , 4)\n",
        "        ious = torch.stack(ious).permute(1 , 0 , 2)\n",
        "        anchors_indices = ious.argsort(descending=True, dim=1)\n",
        "        \n",
        "        for batch in range(roi_imgs.shape[0]):\n",
        "            for filter in range(roi_imgs.shape[1]):\n",
        "                if filter == self.output_len:\n",
        "                    break\n",
        "                anchor = anchors_indices[batch , filter , :]\n",
        "                roi_img = roi_imgs[batch , anchor , : , : , :]\n",
        "                roi_final_imgs.append(roi_img)\n",
        "        roi_final_imgs = torch.stack(roi_final_imgs)\n",
        "        return roi_final_imgs.squeeze(1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWiA_6g3eu1b"
      },
      "source": [
        "roi = ROI().to(device)\n",
        "img = torch.randn(2 , 512 , 50 , 50).to(device)\n",
        "bbox = torch.randn(2 , 5 , 50 , 50 , 5).to(device)\n",
        "target_boxes = torch.randn(2 , 5 , 50 , 50 , 5).to(device)\n",
        "roi_imgs = roi(img , bbox , target_boxes)\n",
        "roi_imgs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6wXqyBVf8P5"
      },
      "source": [
        "class FCN(nn.Module):\n",
        "    def __init__(self ,\n",
        "                 in_channels = 512 , \n",
        "                 out_channels = 512 ,\n",
        "                 hidden_dim = 32):\n",
        "        super(FCN , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , hidden_dim , use_pool=True)\n",
        "        self.conv2 = Conv(hidden_dim , hidden_dim * 2 , use_pool=True)\n",
        "        self.convT1 = ConvT(hidden_dim * 2 , hidden_dim)\n",
        "        self.convT2 = ConvT(hidden_dim , hidden_dim)\n",
        "        self.convT3 = ConvT(hidden_dim , hidden_dim)\n",
        "        self.convT4 = ConvT(hidden_dim , hidden_dim)\n",
        "        self.convT5 = ConvT(hidden_dim , hidden_dim)\n",
        "        self.convT6 = ConvT(hidden_dim , out_channels , padding=7)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.convT1(x)\n",
        "        x = self.convT2(x)\n",
        "        x = self.convT3(x)\n",
        "        x = self.convT4(x)\n",
        "        x = self.convT5(x)\n",
        "        x = self.convT6(x)\n",
        "        return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1nd1KN7f-WM"
      },
      "source": [
        "def test():\n",
        "    x = torch.randn(2 , 512 , 5 , 5).to(device)\n",
        "    fcn = FCN().to(device)\n",
        "    z = fcn(x)\n",
        "    print(z.shape)\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-0efdejkKnY"
      },
      "source": [
        "class Classifier_(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 512 , \n",
        "                 out_channels = 12500):\n",
        "        super(Classifier_ , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , 32 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv2 = Conv(32 , 64 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv3 = Conv(64 , 128 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv4 = Conv(128 , 256 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.conv5 = Conv(256 , 512 , kernel_size=(2 , 2) , stride=(2 , 2) , padding=0)\n",
        "        self.linear1 = Linear(512 , 32 , use_norm=False)\n",
        "        self.linear2 = Linear(32 , out_channels , use_activation=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self , x):  \n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.linear1(x.squeeze(2).squeeze(2))\n",
        "        x = self.sigmoid(self.linear2(x))\n",
        "        return x.view(x.shape[0] , 5 , 50 , 50 , 1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZEJApPvkcNA"
      },
      "source": [
        "x = torch.randn(2 , 1 , 50 , 50).to(device)\n",
        "cls = Classifier_().to(device)\n",
        "z = cls(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPAxbPjLez4a"
      },
      "source": [
        "class Mask_RCNN(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3):\n",
        "        super(Mask_RCNN , self).__init__()\n",
        "\n",
        "        in_channels_vgg = 512\n",
        "        self.vgg = VGG()\n",
        "        self.rpn = RPN(in_channels_vgg)\n",
        "        self.classifier = Classifier(512)\n",
        "        self.roi = ROI()\n",
        "        self.fcn = FCN()\n",
        "        self.to_grayscale = Conv(512 , 1)\n",
        "        self.classifier_ = Classifier_(in_channels=512)\n",
        "\n",
        "    def forward(self , x , target_boxes):\n",
        "        x = self.vgg(x)\n",
        "        cls , bbox = self.rpn(x)\n",
        "        croped_imgs = self.roi(x , bbox , target_boxes)\n",
        "        #print(croped_imgs.shape)\n",
        "        mask_src = self.fcn(croped_imgs)\n",
        "        #\n",
        "        #print(mask_src.shape)\n",
        "        predictions = self.classifier_(mask_src)\n",
        "        mask_src = self.to_grayscale(mask_src)\n",
        "        #predictions = self.classifier_(mask_src)\n",
        "        cls , bbox = self.classifier(croped_imgs)\n",
        "        #print(mask_src.shape)\n",
        "        return cls , bbox , mask_src.view(2 , 5 , 50 , 50 , 1) , predictions"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfBw8I8KfUOE"
      },
      "source": [
        "x = torch.randn(1 , 3 , 800 , 800).to(device)\n",
        "target_boxes = torch.randn(1 , 5 , 50 , 50 , 5).to(device)\n",
        "mask_rcnn = Mask_RCNN().to(device)\n",
        "cls , bbox , mask_src , predictions = mask_rcnn(x , target_boxes)\n",
        "cls.shape , bbox.shape , mask_src.shape , predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBFdQFIzfbaU"
      },
      "source": [
        "adv_criterion = nn.BCEWithLogitsLoss()\n",
        "recon_criterion = nn.L1Loss()\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "lambda_recon = 200\n",
        "betas = (0.5 , 0.999)\n",
        "\n",
        "\n",
        "n_epochs = 200\n",
        "display_steps = 1\n",
        "lr = 0.002"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LwiAZRnEf-I"
      },
      "source": [
        "mask_rcnn = Mask_RCNN().to(device)\n",
        "opt = torch.optim.Adam(mask_rcnn.parameters() , lr=lr , betas = betas)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_rGobpxEp4y"
      },
      "source": [
        "def train():\n",
        "    mean_rcnn_loss = 0\n",
        "    cur_step = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        for img , label ,  mask_label in dataloader:\n",
        "            img , label , mask_label = img.to(device) , label.to(device)  , mask_label.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            cls_ , bbox_ , mask , mask_pred = mask_rcnn(img , label)\n",
        "            #print(cls.shape , bbox.shape)\n",
        "            bbox_loss = recon_criterion(bbox_ , label[...,:4])\n",
        "            cls_loss = recon_criterion(torch.argmax(cls_ , dim=-1).unsqueeze(-1).float() , label[...,4:5].float())\n",
        "            mask_loss = recon_criterion(mask , mask_label[... , :1])\n",
        "            mask_class_loss = recon_criterion(mask_pred , mask_label[... , 1:2])\n",
        "            #print(bbox_loss , cls_loss , mask_loss , mask_class_loss)\n",
        "\n",
        "            loss = (bbox_loss + cls_loss + mask_loss + mask_class_loss) / 4\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            mean_rcnn_loss += loss.item() / display_steps\n",
        "            if cur_step % display_steps == 0:\n",
        "                print(f'Epoch {epoch} , Step {cur_step} , Mean Faster RCNN Loss {mean_rcnn_loss}')\n",
        "            cur_step +=1\n",
        "        mean_rcnn_loss = 0"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaiDNJoRG5Lk"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl7MdotXIMjr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}